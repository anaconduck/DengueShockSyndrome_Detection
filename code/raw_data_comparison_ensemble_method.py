# -*- coding: utf-8 -*-
"""Raw Data_Comparison Ensemble Method.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1anxazxOecpoSISdCToYDSBRziWIMm1zW

# Import Library
"""

!pip install catboost

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
import sys
import warnings
warnings.filterwarnings('ignore')

import pandas as pd
import numpy as np

import matplotlib
from matplotlib import pyplot as plt
# %matplotlib inline
import seaborn as sns

from scipy.stats import spearmanr

from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from collections import Counter
from sklearn.utils import resample
from sklearn.preprocessing import RobustScaler

from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
from catboost import CatBoostClassifier

from sklearn import metrics
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, roc_curve, roc_auc_score

"""# Load Dataset"""

# Mount Google Drive
drive.mount('/content/drive')

# Load the Excel file
df =  pd.read_excel(r'/content/drive/My Drive/ProyekDengue/raws.xlsx')
df_gejala = pd.read_excel(r'/content/drive/My Drive/ProyekDengue/types.xlsx')

df

df.info()

"""# EDA

## Missing Values
"""

# Cek Data Kosong
percent_missing = df.isnull().sum() * 100 / len(df)
missing_value_df = pd.DataFrame({'percent_missing': percent_missing})
missing_value_df

"""## Uni-Variate Analysis

### Jenis Kelamin

**Dalam angka**
"""

df['Gender'].value_counts()

"""**Dalam gambar**"""

plt.figure(figsize=(12,8))

plt.title("Distribution of Gender")
circle = plt.Circle((0, 0), 0.5, color='white')

g = plt.pie(df['Gender'].value_counts(),
            explode=(0.025,0.025),
            labels=['Male','Female'],
            colors=['skyblue','pink'],
            autopct='%1.1f%%',
            startangle=180);

plt.legend()
p = plt.gcf()
p.gca().add_artist(circle)

plt.show()

"""### Usia"""

plt.figure(figsize=(20,10))
sns.countplot(x=df["UMUR"])
plt.title("Distribution of Age",fontsize=20)
plt.xlabel("Age",fontsize=20)
plt.ylabel("Count",fontsize=20)
plt.show()

"""### BB"""

sns.displot(data=df, x="BB", color="magenta")
plt.xlabel("WEIGHT")

plt.title("DISTRIBUTION OF WEIGHT")
plt.show()

"""### HPANAS"""

plt.figure(figsize=(20,10))
sns.countplot(x=df["ONSET of FEVER"])
plt.title("Distribution of Onset Fever",fontsize=20)
plt.xlabel("Starting Day",fontsize=20)
plt.ylabel("Count",fontsize=20)
plt.show()

"""### Gejala Klinis"""

df_gejala.hist(bins=15,
        color='steelblue',
        edgecolor='black',
        linewidth=1.0,
        xlabelsize=8, ylabelsize=8, grid=False)

plt.tight_layout(rect=(0, 0, 1.2, 1.2))

"""### Hemoglobin"""

df1 = df[['HB1','HBSEL']]

df1.hist(bins=15,
        color='steelblue',
        edgecolor='black',
        linewidth=1.0,
        xlabelsize=8, ylabelsize=8, grid=False)

plt.tight_layout(rect=(0, 0, 1.2, 1.2))

"""Pada visualisasi diatas, mayoritas berada pada keadaan normal. Hal ini berdasarkan teori mengenai rentang hemoglobin normal dalam satuan g/dL yang menyatakan sebagai berikut :

- Anak (2-6 tahun): 10.5 - 14.0 g/dL
- Anak (6-12 tahun): 11.5 - 15.5 g/dL
- Remaja (12-17 tahun): 12.0 - 16.0 g/dL

### Trombosit
"""

df2 = df[['TROM1','TROMSEL']]

df2.hist(bins=15,
        color='steelblue',
        edgecolor='black',
        linewidth=1.0,
        xlabelsize=8, ylabelsize=8, grid=False)

plt.tight_layout(rect=(0, 0, 1.2, 1.2))

"""### Hematokrit"""

df2_1 = df[['HCT1','HCTSEL']]

df2_1.hist(bins=15,
        color='steelblue',
        edgecolor='black',
        linewidth=1.0,
        xlabelsize=8, ylabelsize=8, grid=False)

plt.tight_layout(rect=(0, 0, 1.2, 1.2))

"""### Leukosit"""

df3 = df[['LEU','EOSABS','BASABS','NEUABS','LIMABS','MONABS']]

df3.hist(bins=15,
        color='steelblue',
        edgecolor='black',
        linewidth=1.0,
        xlabelsize=8, ylabelsize=8, grid=False)

plt.tight_layout(rect=(0, 0, 1.2, 1.2))

"""### Lainnya"""

df4 = df[['AST','ALT','ALB','UR','CR','PPT','APTT']]

df4.hist(bins=15,
        color='steelblue',
        edgecolor='black',
        linewidth=1.0,
        xlabelsize=8, ylabelsize=8, grid=False)

plt.tight_layout(rect=(0, 0, 1.2, 1.2))

"""### Syock

**Dalam Angka**
"""

df.SYOCK.value_counts()

"""**Dalam Gambar**"""

plt.figure(figsize=(12,8))

plt.title("Distribution of Dengue Shock Syndrome")
circle = plt.Circle((0, 0), 0.5, color='white')
g = plt.pie(df.SYOCK.value_counts(),
            explode=(0.025,0.025),
            labels=['NOT SHOCK','SHOCK'],
            colors=['#0079FF', '#FF0060'],
            autopct='%1.1f%%',
            startangle=180);
plt.legend()
p = plt.gcf()
p.gca().add_artist(circle)

plt.show()

"""## Bi-Variate Analysis

### Gender dengan DSS
"""

plt.figure(figsize=(10,8))
sns.set_theme(style="darkgrid", color_codes=True)
ax = sns.countplot(y="Gender", hue="SYOCK", data=df, palette=['pink','lightgreen'])
plt.yticks([0, 1], ['Female', 'Male'])
plt.title("The Relationship of Dengue Shock Syndrome to Gender")


leg = ax.legend()
leg.set_title('Dengue Shock Syndrome')
leg.get_texts()[0].set_text('SHOCK')
leg.get_texts()[1].set_text('NOT SHOCK')

plt.show()

"""### Usia dengan DSS"""

plt.figure(figsize=(12,10))
sns.set_theme(style="darkgrid", color_codes=True)
ax = sns.countplot(y="UMUR", hue="SYOCK", data=df, palette=['pink','lightgreen'])
ax.set_ylabel("Age")
plt.title("The Relationship of Dengue Shock Syndrome to Age")

leg = ax.legend()
leg.set_title('Dengue Shock Syndrome')
leg.get_texts()[0].set_text('SHOCK')
leg.get_texts()[1].set_text('NOT SHOCK')

plt.show()

"""### Gejala Klinis dengan DSS"""

cat_col = ['anaemia','diabetes','high_blood_pressure','gender','smoking']

plt.figure(figsize=(15,15))

for i, col in enumerate(cat_col):
    plt.subplot(2,3,i+1)
    plt.title(col, fontsize=13)
    plt.xlabel(col, fontsize=12)
    plt.ylabel("count",fontsize=12)
    plt.subplots_adjust(hspace=0.5, wspace=0.3)
    ax = sns.countplot(data=df, x=col, hue='death_status',palette=['pink','lightgreen'])
    for i in ax.patches:
        ax.annotate(format(i.get_height()), (i.get_x() + i.get_width()/2.,i.get_height()),
               ha='center',va='center',xytext=(0,7), textcoords='offset points')

"""## Multi Variate Analysis"""

corr = df.corr()
plt.figure(figsize=(50,50))
# sns.heatmap(corr, cmap='PuBu', annot=True)
sns.heatmap(corr, annot=True, cmap='PuBu',
            annot_kws={'fontsize':11},
            linewidths=0.01,linecolor="white");

plt.figure(figsize=(12, 14))
heatmap =sns.heatmap(df.corr()[['SYOCK']].sort_values(by='SYOCK', ascending=False), vmin=-1, vmax=1, annot=True, cmap='RdYlGn')
heatmap.set_title('Fitur yang berkorelasi dengan fitur (SYOCK)', fontdict={'fontsize':18}, pad=16);

# Spearman
spearman_correlations = df.corrwith(df["SYOCK"], method='spearman').sort_values(ascending=False)
print(spearman_correlations)

"""1. Fitur dengan Korelasi Positif yang Tinggi:

- EFUSIPLEURA (0.526)
- HEPATOMEGALI (0.421)
- VOMITING (0.384)
- ENSEFALOPATI (0.323)
- NAUSEA (0.319)
- EPIGASTRIUM (0.314)

Fitur-fitur ini memiliki korelasi positif yang tinggi dengan target dan mungkin berkontribusi signifikan dalam memprediksi hasil.


2. Fitur dengan Korelasi Negatif yang Tinggi:

- UR (-0.336)
- AST (-0.176)
- ALT (-0.156)

Meskipun korelasi negatif, fitur-fitur ini mungkin tetap relevan dan berkontribusi pada pemodelan.

3. Fitur dengan Korelasi Rendah atau Dekat Nol:

- BB (0.009)
- BAS (0.001)
- ONSET of FEVER (0.000)

Fitur-fitur ini memiliki korelasi rendah atau sangat dekat dengan nol, dan mungkin tidak memberikan kontribusi yang signifikan.

# Preprocessing
"""

df_copy = df.copy()

"""## Handling Missing Values

**Drop Kolom dengan Missing Value > 55% dan Pemeriksaan Kedua pada HCT & HB & TROM**
"""

# df_copy = df_copy.drop(['ALB', 'UR', 'CR', 'PPT', 'APTT'], axis=1)

"""**Impute Missing Value**"""

from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.ensemble import RandomForestRegressor

imptr = IterativeImputer(RandomForestRegressor(), max_iter=10, random_state=0)

data = pd.DataFrame(imptr.fit_transform(df_copy), columns = df_copy.columns)

data.head()

"""## Handling Outliers"""

# Q1
q1 = data.quantile(0.25)
# Q3
q3 = data.quantile(0.75)
# IQR
IQR = q3 - q1
# Outlier range
upper = q3 + IQR * 1.5
lower = q1 - IQR * 1.5
upper_dict = dict(upper)
lower_dict = dict(lower)

for i,v in data.items():
    v_col = v[( v<= lower_dict[i]) | (v >= upper_dict[i])]
    perc = np.shape(v_col)[0] * 100.0 / np.shape(data)[0]
    print("Column {} outliers = {} => {}%".format(i,len(v_col),round((perc),3)))

"""## Feature Selection"""

plt.figure(figsize=(12, 14))

heatmap =sns.heatmap(data.corr()[['SYOCK']].sort_values(by='SYOCK', ascending=False), vmin=-1, vmax=1, annot=True, cmap='RdYlGn')
heatmap.set_title('Fitur yang berkorelasi dengan fitur (SYOCK)', fontdict={'fontsize':18}, pad=16);

data = data.drop(['SPLENOMEGALI', 'ASITES', 'ENSEFALOPATI', 'ENSEFALITIS', 'KIDNEY_FAILURE', 'HEPATIC_FAILURE', 'MYOCARDITIS'], axis=1)

df_fix = data[['EFUSIPLEURA',
'HEPATOMEGALI',
'VOMITING',
'NAUSEA',
'EPIGASTRIUM',
'SYOCK']]

df_fix.head()

"""## Skewness Check

**Cek Dulu**
"""

from scipy.stats import skew

cols = df_fix.loc[:, df_fix.columns != 'SYOCK'].columns.tolist()
num_cols = len(cols)

num_rows = (num_cols + 2) // 3

plt.figure(figsize=(15, 5 * num_rows))
for i, col in enumerate(cols):
    plt.subplot(num_rows, 3, i + 1)
    plt.title(col, fontsize=13)
    plt.xlabel(f"skew of {col}:{skew(df_fix[col])}")
    plt.ylabel("count", fontsize=12)
    plt.subplots_adjust(hspace=0.5, wspace=0.3)
    sns.histplot(data=df_fix, x=col, bins=50, palette=['red', 'green'])

"""Keterangan Skewness:

- Cukup simetris (fairly symmetrical) : -0.5 to 0.5
- Kemiringan sedang (moderate skewed) : -0.5 to -1.0 and 0.5 to 1.0
- Kemiringan berat (highly skewed) : <-1.0 and > 1.0

# Split Dataset
"""

X = df_fix.iloc[:, :-1]
y = df_fix['SYOCK']

"""### Normal (Imbalanced Data)"""

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.25,random_state=42)

"""### SMOTE"""

print("Before oversampling: ",Counter(y))
smote = SMOTE()
X_smote, y_smote = smote.fit_resample(X, y)

print("After oversampling: ",Counter(y_smote))
Xtrain_smote, Xtest_smote, ytrain_smote, ytest_smote = train_test_split(X_smote,y_smote,test_size=0.2, random_state=42)

# df_major = df[df.Activity==0]
# df_minor = df[df.Activity!=0]

# df_downsampled = resample(df_major, n_samples=30000, random_state=42)
# df = pd.concat([df_downsampled, df_minor])
# df.Activity.value_counts()

"""# Robust Scaler atau Min Max Scaler

"""

# from sklearn.preprocessing import RobustScaler
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()

#Normal
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

#SMOTE
X_train_scaled_smote = scaler.fit_transform(Xtrain_smote)
X_test_scaled_smote = scaler.fit_transform(Xtest_smote)

"""# Modeling

Dalam proses modeling, diterapkan tiga algoritma ensemble method yang berbeda: Random Forest, XGBoost, dan CatBoost. Pendekatan ensemble ini memungkinkan untuk memanfaatkan kekuatan masing-masing algoritma, menciptakan model yang lebih kuat, dan meningkatkan akurasi prediksi kami dalam berbagai tugas.
"""



def print_score(clf, X_train, y_train, X_test, y_test, train=True):
    if train:
        pred = clf.predict(X_train)
        clf_report = pd.DataFrame(classification_report(y_train, pred, output_dict=True))
        print("Train Result:\n================================================")
        print(f"Accuracy Score: {accuracy_score(y_train, pred) * 100:.2f}%")
        print("_______________________________________________")
        print(f"CLASSIFICATION REPORT:\n{clf_report}")
        print("_______________________________________________")
        print(f"Confusion Matrix: \n {confusion_matrix(y_train, pred)}\n")

    elif train==False:
        pred = clf.predict(X_test)
        clf_report = pd.DataFrame(classification_report(y_test, pred, output_dict=True))

        tn, fp, fn, tp = confusion_matrix(y_test,pred).ravel()

        sensitivity = tp / (tp + fn)
        specificity = tn / (tn + fp)

        ppv = tp / (tp + fp)
        npv = tn / (tn + fn)

        print("Test Result:\n================================================")
        print(f"Accuracy Score: {accuracy_score(y_test, pred) * 100:.2f}%")
        print("Sensitivity:", sensitivity)
        print("Specificity:", specificity)
        print("PPV:", ppv)
        print("NPV:", npv)
        print("_______________________________________________")
        print(f"CLASSIFICATION REPORT:\n{clf_report}")
        print("_______________________________________________")
        print(f"Confusion Matrix: \n {confusion_matrix(y_test, pred)}\n")

"""## Random Forest

### Default
"""

RF = RandomForestClassifier()
RF.fit(X_train_scaled, y_train)

print_score(RF, X_train_scaled, y_train, X_test_scaled, y_test, train=False)

"""### SMOTE"""

RF_smote = RandomForestClassifier()
RF_smote.fit(X_train_scaled_smote, ytrain_smote)

print_score(RF_smote, X_train_scaled_smote, ytrain_smote, X_test_scaled_smote, ytest_smote, train=False)

"""### Hyperparameter Tuning"""

plt.figure(figsize=(10, 6))

y_scor = RF.predict_proba(X_test_scaled)[:, 1]
fpr_svm, tpr_svm, _ = roc_curve(y_test, y_scor)
auc_svm = roc_auc_score(y_test, y_scor)
plt.plot(fpr_svm, tpr_svm, label='SVM (AUC = {:.2f})'.format(auc_svm))

plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Random Forest')
plt.legend(loc='lower right')
plt.show()

"""## XGBoost

### Default
"""

model = xgb.XGBClassifier()
model.fit(X_train_scaled, y_train)

print_score(model, X_train_scaled, y_train, X_test_scaled, y_test, train=False)

"""### SMOTE"""

model_smote = xgb.XGBClassifier()
model_smote.fit(X_train_scaled_smote, ytrain_smote)

print_score(RF_smote, X_train_scaled_smote, ytrain_smote, X_test_scaled_smote, ytest_smote, train=False)

"""### Hyperparameter Tuning"""

model_tuning = xgb.XGBClassifier(n_estimators = 200, learning_rate = 0.1, max_depth = 3)
model_tuning.fit(X_train_scaled, y_train)

print_score(model_tuning, X_train_scaled, y_train, X_test_scaled, y_test, train=False)

plt.figure(figsize=(10, 6))

y_score = model_tuning.predict_proba(X_test_scaled)[:, 1]
fpr_svm, tpr_svm, _ = roc_curve(y_test, y_score)
auc_svm = roc_auc_score(y_test, y_score)
plt.plot(fpr_svm, tpr_svm, label='SVM (AUC = {:.2f})'.format(auc_svm))

plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('XGBoost')
plt.legend(loc='lower right')
plt.show()

from sklearn.model_selection import GridSearchCV

# Inisialisasi XGBoost Classifier
model_tuning = xgb.XGBClassifier()

# Daftar hyperparameter yang akan dituning
param_grid = {
    'learning_rate': [0.1, 0.01, 0.001],
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7],
    'min_child_weight': [1, 3, 5],
    'gamma': [0, 0.1, 0.2],
    'subsample': [0.8, 0.9, 1.0],
    'colsample_bytree': [0.8, 0.9, 1.0],
    'scale_pos_weight': [1, 2, 3]
}

# Inisialisasi GridSearchCV
grid_search = GridSearchCV(
    estimator=model_tuning,
    param_grid=param_grid,
    scoring='accuracy',
    cv=5,  # Jumlah fold cross-validation
    n_jobs=-1  # Menggunakan semua core CPU yang tersedia
)

# Melakukan tuning pada dataset
grid_result = grid_search.fit(X_train_scaled, y_train)

# Menampilkan parameter terbaik yang ditemukan
print("Best Parameters:", grid_result.best_params_)

# Melakukan prediksi dengan model terbaik
y_pred = grid_result.predict(X_test_scaled)

# Mengukur akurasi model
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

"""## CatBoost

### Default
"""

model_cat = CatBoostClassifier()
model_cat.fit(X_train_scaled, y_train)

print_score(model_cat, X_train_scaled, y_train, X_test_scaled, y_test, train=False)

"""### SMOTE"""

model_cat_smote = CatBoostClassifier()
model_cat_smote.fit(X_train_scaled_smote, ytrain_smote)

print_score(model_cat_smote, X_train_scaled_smote, ytrain_smote, X_test_scaled_smote, ytest_smote, train=False)

"""### Hyperparameter Tuning"""

model_catt = CatBoostClassifier(n_estimators=800 , depth = 3)
model_catt.fit(X_train_scaled, y_train)

print_score(model_cat, X_train_scaled, y_train, X_test_scaled, y_test, train=False)

plt.figure(figsize=(10, 6))

y_scores = model_catt.predict_proba(X_test_scaled)[:, 1]
fpr_svm, tpr_svm, _ = roc_curve(y_test, y_scores)
auc_svm = roc_auc_score(y_test, y_scores)
plt.plot(fpr_svm, tpr_svm, label='SVM (AUC = {:.2f})'.format(auc_svm))

plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('CatBoost')
plt.legend(loc='lower right')
plt.show()

"""# Conclusion

Berdasarkan hasil perbandingan ensemble method antara random forest, xgboost, dan catboost, dapat disimpulkan bahwa model XGBoost menunjukkan kinerja yang superior dibandingkan dua model lainnya. Dengan akurasi sebesar 90,2%, XGBoost menjadi pilihan yang sangat baik untuk proyek ini. Meskipun demikian, penting untuk mengingat bahwa performa model dapat dipengaruhi oleh konteks dan karakteristik data tertentu. Oleh karena itu, rekomendasi untuk mengimplementasikan model ini secara luas perlu disesuaikan dengan kebutuhan dan tujuan bisnis spesifik.
"""